{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path\n",
    "    ):\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        self.dataset = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Reads item either from memory or from disk\"\"\"\n",
    "        X = self.dataset[index]\n",
    "        return X, y\n",
    "\n",
    "class LitDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        fnames, labels = XX\n",
    "        self.trainset = Dataset(\n",
    "            fnames[\"train\"],\n",
    "            labels[\"train\"],\n",
    "            preload_transform=None,\n",
    "            transform=self.tf_train,\n",
    "            load_to_memory=self.load_to_memory,\n",
    "        )\n",
    "\n",
    "        self.valset = Dataset(\n",
    "            fnames[\"val\"],\n",
    "            labels[\"val\"],\n",
    "            preload_transform=None,\n",
    "            transform=self.tf_test,\n",
    "            load_to_memory=self.load_to_memory,\n",
    "        )\n",
    "\n",
    "        self.testset = Dataset(\n",
    "            fnames[\"test\"],\n",
    "            labels[\"test\"],\n",
    "            preload_transform=None,\n",
    "            transform=self.tf_test,\n",
    "            load_to_memory=self.load_to_memory,\n",
    "        )\n",
    "\n",
    "        tta_list = [self.testset] + [\n",
    "            Dataset(\n",
    "                fnames[\"test\"],\n",
    "                labels[\"test\"],\n",
    "                preload_transform=None,\n",
    "                transform=self.tf_train,\n",
    "                load_to_memory=self.load_to_memory,\n",
    "            )\n",
    "            for _ in range(self.tta_n - 1)\n",
    "        ]\n",
    "\n",
    "        self.ttaset = torch.utils.data.ConcatDataset(tta_list)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=self.drop_last(self.trainset),\n",
    "            num_workers=self.cpu_count,\n",
    "        )\n",
    "\n",
    "        return trainloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valloader = torch.utils.data.DataLoader(\n",
    "            self.valset,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=self.drop_last(self.valset),\n",
    "            num_workers=self.cpu_count,\n",
    "        )\n",
    "\n",
    "        return valloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "            self.testset,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            num_workers=self.cpu_count,\n",
    "        )\n",
    "\n",
    "        return testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning module for training an arbitary model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        freeze_base: bool = False,\n",
    "        pretrained: bool = True,\n",
    "        n_classes: int = 1,\n",
    "        criterion: str = \"mse\",\n",
    "        opt: dict = {\"name\": \"adam\"},\n",
    "        lr: float = 1e-4,\n",
    "        label_transform=None,\n",
    "    ):\n",
    "        \"\"\"Initialize the module\n",
    "        Args:\n",
    "            model (str): name of the ResNet model to use\n",
    "\n",
    "            freeze_base (bool): whether to freeze the base model\n",
    "\n",
    "            pretrained (bool): whether to use pretrained weights\n",
    "\n",
    "            n_classes (int): number of outputs. Set 1 for regression\n",
    "\n",
    "            criterion (str): loss function to use\n",
    "\n",
    "            lr (float): learning rate\n",
    "\n",
    "            label_transform: possible transform that is done for the output labels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"label_transform\"])\n",
    "        self.example_input_array = torch.randn((1, 3, 224, 224))\n",
    "        self.model = Model(\n",
    "            model=model,\n",
    "            freeze_base=freeze_base,\n",
    "            pretrained=pretrained,\n",
    "            n_classes=n_classes,\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.label_transform = label_transform\n",
    "        self.criterion = choose_criterion(criterion)\n",
    "        self.opt_args = opt\n",
    "\n",
    "        if criterion == \"cross-entropy\":\n",
    "            self.is_classifier = True\n",
    "        else:\n",
    "            self.is_classifier = False\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def predict_func(self, output):\n",
    "        \"\"\"Processes the output for prediction\"\"\"\n",
    "        if self.is_classifier:\n",
    "            return output.argmax(dim=1)\n",
    "        else:\n",
    "            return output.flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Sets optimizers based on a dict passed as argument\"\"\"\n",
    "        if self.opt_args[\"name\"] == \"adam\":\n",
    "            return torch.optim.Adam(self.model.parameters(), self.lr)\n",
    "        elif self.opt_args[\"name\"] == \"adamw\":\n",
    "            return torch.optim.AdamW(self.model.parameters(), self.lr)\n",
    "        else:\n",
    "            raise Exception(\"Invalid optimizer\")\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.criterion(out, y)\n",
    "        return x, y, out, loss\n",
    "\n",
    "    def common_epoch_end(self, outputs, name: str):\n",
    "        \"\"\"Combination of outputs for calculating metrics\"\"\"\n",
    "        y_true = torch.cat([x[\"y_true\"] for x in outputs]).cpu().detach().numpy()\n",
    "        y_pred = torch.cat([x[\"y_pred\"] for x in outputs]).cpu().detach().numpy()\n",
    "\n",
    "        if self.label_transform:\n",
    "            y_true = self.label_transform(y_true)\n",
    "            y_pred = self.label_transform(y_pred)\n",
    "\n",
    "        if self.is_classifier:\n",
    "            self.log(f\"{name}/acc\", accuracy_score(y_true, y_pred))\n",
    "            self.log(\n",
    "                f\"{name}/f1\",\n",
    "                f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
    "            )\n",
    "\n",
    "        return y_true, y_pred\n",
    "\n",
    "    # Training\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, y, out, loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        outputs = {\"loss\": loss, \"y_true\": y, \"y_pred\": self.predict_func(out)}\n",
    "        self.training_step_outputs.append(outputs)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = self.training_step_outputs\n",
    "        _, _ = self.common_epoch_end(outputs, \"train\")\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    # Validation\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, y, out, val_loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"val/loss\", val_loss, on_step=True, on_epoch=True)\n",
    "        outputs = {\"y_true\": y, \"y_pred\": self.predict_func(out)}\n",
    "        self.validation_step_outputs.append(outputs)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        _, _ = self.common_epoch_end(outputs, \"val\")\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    # Testing\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, y, out, test_loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"test/loss\", test_loss, on_step=True, on_epoch=True)\n",
    "        outputs = {\"y_true\": y, \"y_pred\": self.predict_func(out), \"out\": out}\n",
    "        self.test_step_outputs.append(outputs)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        outputs = self.test_step_outputs\n",
    "        if self.is_classifier:\n",
    "            self.softmax = (\n",
    "                torch.cat([x[\"out\"] for x in outputs])\n",
    "                .softmax(dim=1)\n",
    "                .cpu()\n",
    "                .detach()\n",
    "                .numpy()\n",
    "            )\n",
    "        self.y_true, self.y_pred = self.common_epoch_end(outputs, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
